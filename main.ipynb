{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Reinforcement Learning Assignment**\n",
        "\n",
        "Your task is to complete the implementation of the value iteration algorithm in the code below.\n",
        "\n",
        "Steps:\n",
        "- Read the code carefully to get an idea of how it works.\n",
        "- Run all the cells. You should get an output that shows more losses than wins like this:\n",
        "\n",
        "```\n",
        "Results after 1000 simulations: {'win': 310, 'loss': 690, 'draw': 0}\n",
        "\n",
        "```\n",
        "\n",
        "- Implement the `value_iteration` function without changing its signature or its return type.\n",
        "\n",
        "- After a successful implementation of the value iteration algorithm, your results should show **more wins than losses** most of the time we run the algorithm. You should get an output like this:\n",
        "\n",
        "```\n",
        "Results after 1000 simulations: {'win': 478, 'loss': 448, 'draw': 74}\n",
        "\n",
        "Results after 1000 simulations: {'win': 502, 'loss': 425, 'draw': 73}\n",
        "\n",
        "```\n",
        "\n",
        "**Notes:**\n",
        "- ou may change the default values of \"theta\" and \"gamma\" in the `value_iteration` function, but do not change the signature or return type.\n",
        "\n",
        "**Submission:**\n",
        "- Submit your notebook link and the number of wins and losses via Gradescope.\n",
        "\n",
        "\n",
        "**AI Policy:**\n",
        "- The use of any artificial intelligence (AI) or code generation tools in this assignment is **strictly prohibited**. Violating this rule may result in your disenrollment from this course. You may be required to orally defend your submission. Failure to satisfactorily explain your work will result in a zero for the assignment, and you may fail the course."
      ],
      "metadata": {
        "id": "vZnmvzdYk2qo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arnRapd1rLYO"
      },
      "outputs": [],
      "source": [
        "!pip install matplot\n",
        "!pip install numpy\n",
        "!pip install seaborn\n",
        "!pip install tqdm\n",
        "!pip install gymnasium\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "HIT = 1 # deal another card / draw\n",
        "STICK = 0 # stop dealing cards\n",
        "\n",
        "# each state is a tuple of (player_value, dealer_facing_card_value, usable_ace)\n",
        "actions = [HIT, STICK]\n",
        "\n",
        "states = []\n",
        "for player_value in range(4, 22):\n",
        "    for dealer_facing_card_value in range(1, 11):\n",
        "        for usable_ace in [True, False]:\n",
        "            states.append((player_value, dealer_facing_card_value, usable_ace))\n",
        "\n",
        "transition_probabilities = {}\n",
        "for state in states:\n",
        "    for action in actions:\n",
        "        for next_state in states:\n",
        "            if state[0] >= next_state[0]:\n",
        "                transition_probabilities[(state, action, next_state)] = 0\n",
        "            else:\n",
        "                transition_probabilities[(state, action, next_state)] = 0.0025\n",
        "rewards = {}\n",
        "for state in states:\n",
        "    for action in actions:\n",
        "        for next_state in states:\n",
        "            if state[0] == 21:  # Simplified condition for winning\n",
        "                rewards[(state, STICK, next_state)] = 1\n",
        "                rewards[(state, HIT, next_state)] = -1\n",
        "            elif next_state[0] == 21:\n",
        "                rewards[(state, action, next_state)] = 1\n",
        "            else:\n",
        "                rewards[(state, action, next_state)] = -0.5\n",
        "\n",
        "def value_iteration(states, actions, transition_probabilities, rewards, discount_factor=0.99, theta=0.001):\n",
        "    # Initialize V-values of the states\n",
        "    V = {state: 0 for state in states}\n",
        "\n",
        "    while True:\n",
        "        delta = 0\n",
        "        # For each state, calculate the new value based on the Bellman equation\n",
        "        for state in states:\n",
        "            previous_value = V[state]\n",
        "            max_expected_value  = float('-inf')\n",
        "\n",
        "             # Evaluate each possible action\n",
        "            for action in actions:\n",
        "                value = 0\n",
        "                # Calculate the expected value for each possible next state\n",
        "                for next_state in states:\n",
        "                    value += transition_probabilities[(state, action, next_state)] * \\\n",
        "                             (rewards[(state, action, next_state)] + discount_factor * V[next_state])\n",
        "                max_expected_value  = max(max_expected_value , value)\n",
        "            V[state] = max_expected_value\n",
        "            delta = max(delta, abs(previous_value - V[state]))\n",
        "        if delta < theta:\n",
        "            break\n",
        "\n",
        "    return V\n",
        "\n",
        "\n",
        "\n",
        "# Run value iteration\n",
        "V = value_iteration(states, actions, transition_probabilities, rewards)\n",
        "\n",
        "# Display some of the results\n",
        "for state in [(12, 1, True), (21, 10, False), (15, 5, True)]:  # Example states\n",
        "    print(f\"Value of state {state}: {V[state]}\")\n",
        "\n",
        "\n",
        "# for state in states:\n",
        "#     if V[state] > 0:\n",
        "#         print(f\"Value of state {state}: {V[state]}\")\n",
        "\n",
        "import random\n",
        "def draw_card():\n",
        "    \"\"\"Draw a card with values between 1 and 10, simulating a simplified deck.\"\"\"\n",
        "    return min(random.randint(1, 13), 10)\n",
        "\n",
        "def use_policy(state, V):\n",
        "    \"\"\"Decide action based on the policy derived from V.\"\"\"\n",
        "    hit_value = V.get((state[0] + draw_card(), state[1], state[2]), 0)\n",
        "    stick_value = V.get((state[0], state[1], state[2]), 0)\n",
        "    return HIT if hit_value > stick_value else STICK\n",
        "\n",
        "def simulate_game(V):\n",
        "    \"\"\"Simulate a single game of blackjack based on the policy derived from V.\"\"\"\n",
        "    player_value, dealer_value = draw_card(), draw_card()\n",
        "    usable_ace = player_value == 1\n",
        "    if usable_ace: player_value += 10  # Simplify ace handling: always count as 11 when first drawn\n",
        "\n",
        "    # Player's turn\n",
        "    while True:\n",
        "        action = use_policy((player_value, dealer_value, usable_ace), V)\n",
        "        if action == STICK or player_value >= 21:\n",
        "            break\n",
        "        card = draw_card()\n",
        "        if card == 1 and player_value <= 10:\n",
        "            player_value += 11  # Simplify ace handling: count as 11 if beneficial\n",
        "            usable_ace = True\n",
        "        else:\n",
        "            player_value += card\n",
        "\n",
        "    # Dealer's turn\n",
        "    while dealer_value < 17:\n",
        "        dealer_value += draw_card()\n",
        "        if dealer_value > 21:  # Dealer busts\n",
        "            return 'win'\n",
        "\n",
        "    # Determine outcome\n",
        "    if player_value > 21 or (player_value < dealer_value and dealer_value <= 21):\n",
        "        return 'loss'\n",
        "    elif player_value == dealer_value:\n",
        "        return 'draw'\n",
        "    else:\n",
        "        return 'win'\n",
        "\n",
        "# Run simulation 1000 times\n",
        "results = {'win': 0, 'loss': 0, 'draw': 0}\n",
        "for _ in range(1000):\n",
        "    result = simulate_game(V)\n",
        "    results[result] += 1\n",
        "\n",
        "print(f\"Results after 1000 simulations: {results}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZKEgXjFNyOp",
        "outputId": "8821ba42-77da-4fae-a605-7ae1c6b6b4bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value of state (12, 1, True): -0.16471004043361878\n",
            "Value of state (21, 10, False): 0.0\n",
            "Value of state (15, 5, True): -0.07434060389623591\n",
            "Results after 1000 simulations: {'win': 524, 'loss': 405, 'draw': 71}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}